# ============================
# –ü–æ–∏—Å–∫ —Å—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
# ============================
import numpy as np
from sklearn.preprocessing import normalize
from loguru import logger


def find_similar_documents(query, topic_model, index, df, top_k=3):
    logger.info(f"–ü–æ–∏—Å–∫ —Å—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
    query_probabilities = topic_model.transform([query])[1][0]  # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    query_vector = normalize(np.array([query_probabilities]))
    distances, indices = index.search(query_vector, top_k)
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(indices[0])} –ø–æ—Ö–æ–∂–∏—Ö —ç–∫—Å–ø–æ–Ω–∞—Ç–æ–≤.")
    results = []
    for idx in indices[0]:
        row = df.iloc[idx]
        results.append({
            "name": row['title'],
            "url": row['url'],
            "authors": row['authors'],
            "description": row['description'][:200]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤
        })
    return results


def find_exhibitions(df, top_k=3):
    """
    –ü–æ–∏—Å–∫ —Ç–æ–ø-K –≤—ã—Å—Ç–∞–≤–æ–∫ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –º—É–∑–µ—è (–∫–æ–ª–ª–µ–∫—Ü–∏–∏).
    """
    logger.info(f"–ü–æ–∏—Å–∫ —Ç–æ–ø-{top_k} –≤—ã—Å—Ç–∞–≤–æ–∫...")

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—ã—Å—Ç–∞–≤–∫–∞–º –∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º
    top_exhibitions = (
        df.groupby(['exhibition', 'collection'])
        .size()
        .sort_values(ascending=False)
        .head(top_k)
        .reset_index()
    )

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    return [
        f"üîπ {row['exhibition']}, {row['collection'] or '–ú—É–∑–µ–π –Ω–µ —É–∫–∞–∑–∞–Ω'}"
        for _, row in top_exhibitions.iterrows()
    ]
